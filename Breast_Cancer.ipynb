{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports des bibliothèques\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1- Importation des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt, log\n",
    "\n",
    "# chargement des données\n",
    "breast_DF = pd.read_csv('breast-cancer-wisconsin.data', sep = ',', header = None)\n",
    "\n",
    "# Renommage des colonnes \n",
    "breast_DF.columns = ['ID', \n",
    "                    'Clump_Thickness', 'Uniformity_of_Cell_Size', 'Uniformity_of_Cell_Shape','Marginal_Adhesion', \n",
    "                    'Single_Epithelial_Cell_Size','Bare_Nuclei','Bland_Chromatin','Normal_Nucleoli','Mitoses',\n",
    "                    'Class']\n",
    "\n",
    "#affichage de quelques tuples\n",
    "breast_DF.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_DF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Attribute Information:</h2>\n",
    "-- 1. ID: est l'identifiant d'une instance (Sample code number)<br>\n",
    "-- 2. Clump_Thickness: Épaisseur de touffe prend des valeurs entre 1 - 10<br>\n",
    "-- 3. Uniformity_of_Cell_Size: Uniformité de la taille des cellules prend des valeurs entre 1 - 10<br>\n",
    "-- 4. Uniformity_of_Cell_Shape: Uniformité de la forme des cellules prend des valeurs entre 1 - 10<br>\n",
    "-- 5. Marginal_Adhesion:Adhérence marginale prend des valeurs entre 1 - 10<br>\n",
    "-- 6. Single_Epithelial_Cell_Size: Taille de cellule épithéliale unique prend des valeurs entre 1 - 10<br>\n",
    "-- 7. Bare_Nuclei: Un noyau dans une préparation cytologique qui est pratiquement dépourvu de cytoplasme qui prend des valeurs entre  1 - 10<br>\n",
    "-- 8. Bland_Chromatin:Chromatine fade qui prend des valeurs  entre 1 - 10<br>\n",
    "-- 9. Normal_Nucleoli:Normal Nucléoles qui prend des valeurs entre 1 - 10<br>\n",
    "-- 10. Mitoses: 1 - 10<br>\n",
    "-- 11. Class: (2 for benign, 4 for malignant)<br>                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Transformation de '?' vers NaN et detection des valeurs manquantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_DF = breast_DF.replace('?',np.nan)\n",
    "breast_DF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_count = breast_DF.isnull().sum()\n",
    "na_columns = list(na_count[na_count>0].index.values)\n",
    "\n",
    "print(\"Colonnes avec les valeurs manquantes:\")\n",
    "print(na_columns)\n",
    "\n",
    "na_count.plot(kind = \"bar\", title=\"Plot bar de nombres de valeurs manquantes pour chaque attribut\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'apres les deux derniers bouts de code, on aperçoit qu'on a des données manquantes sur la colonne \"Bare_Nuclei\" ou on a 16 valeurs vides manquantes non renseignés, qu'on va par la suite traiter dans une prochaine partie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  2- Compréhension des données\n",
    "<br>\n",
    "Etude statistique avec des nombres qui vont resumé les proprietés des données qu'on étudie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1 Mesures de tendance centrale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Weighted arithmetic mean:\n",
    "means = breast_DF.drop(['ID','Class'], axis=1)\n",
    "means.mean(skipna=True).plot(kind='bar',title=\"Plot bar des moyennes pour chaque attribut du dataset sans ID et Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trimmed MEAN:\n",
    "trimmed=(means.sum() - means.min() - means.max()) / (means.notnull().sum() - 2)\n",
    "trimmed.plot(kind='bar',title=\"Plot bar des moyennes trimmed pour chaque attribut du dataset sans ID et Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Median\n",
    "means.median().plot(kind='bar',title=\"Plot bar des mediannes pour chaque attribut du dataset sans ID et Class\")\n",
    "means.median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mode: Le mode d'un ensemble de valeurs est la valeur qui apparaît le plus souvent. Il peut s'agir de plusieurs valeurs.\n",
    "means.mode(dropna=True).plot(kind='bar',title=\"Plot bar des modes pour chaque attribut du dataset sans ID et Class\")\n",
    "means.mode(dropna=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX \n",
    "print(\"Max de chaque attribut:\")\n",
    "means.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min\n",
    "print(\"Min de chaque attribut:\")\n",
    "means.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution Gaussienne\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(means, alpha=0.2, figsize=(18, 18), diagonal='kde')\n",
    "#plt.savefig(\"Distribution des attributs.png\") \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de cette matrix scatter plot, on aperçoit que les attributs suivants ne sont pas en distrubution gaussienne, par la suite il faut les transformer, on verra çà dans une prochaine étape dans la data transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Breast cancer disease\n",
    "# Valeur de Class 2= non malade sinon 4=  malade\n",
    "malade = (breast_DF['Class'] == 2)\n",
    "print(\"Nombre de gens atteintes de cancer breast Disease:\",malade.sum())\n",
    "nonMalade = (breast_DF['Class'] == 4)\n",
    "print(\"Nombre de gens non atteintes de cancer breast Disease:\",nonMalade.sum())\n",
    "\n",
    "d = {'Malade': [malade.sum()], 'Non Malade': [nonMalade.sum()]}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "df.plot(kind = \"bar\", title=\"Plot bar de nombres de Malade et non Malade de breast cancer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On apperçoit à partir de ce petit test et de ce plot, que les classes sont imbalanced; ça implique que le dataset est imbalanced, et qu'on a besoin de traiter ce type de probleme qu'on vas voir dans les prochaines étapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2 Mesures de la dispersion des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quartiles et outliers\n",
    "means['Bare_Nuclei'] = pd.to_numeric(means['Bare_Nuclei'])\n",
    "means.boxplot(figsize=(20,5))\n",
    "#plt.savefig(\"Boxplot des attributs.png\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de ce plot on apperçoit qu'ils ne y'a pas de vrai outliers meme ci qu'on a quelques données qui appartiennent pas aux differents Boxplots, et celà revient au faite que les instances prennent des valeurs de 1 à 10 pour les valeurs attributs de chaque row, donc celà nous évite de travailler sur les problemes outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quantiles\n",
    "#Q1 et Q2 et Q3\n",
    "print(\"Les quantiles du dataset\")\n",
    "x = means.quantile([0.25,0.5,0.75])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.plot(figsize=(10,5),title=\"Graphe affichant les differents quantiles des attributs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interquartiles\n",
    "def find_iqr(x):\n",
    "  return np.subtract(*np.percentile(x, [75, 25]))\n",
    "\n",
    "print(\"Les interquartiles du dataset sont:\")\n",
    "means.apply(find_iqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Variance and standard deviation \n",
    "#Variance\n",
    "print(\"La variance des attributs du data set\")\n",
    "print(means.var())\n",
    "print()\n",
    "#Standard deviation\n",
    "print(\"La deviation standard des attributs du data set\")\n",
    "print(means.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  3-  Pre-processing des données\n",
    "## 3-1 Data cleaning\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme c'était le cas à partir de la partie de data understanding (section 2), on a été confronté à beaucoup de problemes, notamment le probleme de des valeurs manquantes sur la colonne \"Bare_Nuclei\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing values problem resolution \n",
    "missing_DF = breast_DF.drop(['ID'], axis=1)\n",
    "\n",
    "\n",
    "print('Nombre de valeurs manquantes pour chaque attribut de dataset:')\n",
    "for colonne in missing_DF.columns:\n",
    "    print('\\t%s: %d' % (colonne,missing_DF[colonne].isna().sum()))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before imputation\n",
    "missing_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import isnan\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# define imputer\n",
    "imputer = KNNImputer()\n",
    "# fit on the dataset\n",
    "imputer.fit(missing_DF)\n",
    "# transform the dataset\n",
    "Xtrans = imputer.transform(missing_DF)\n",
    "# print total missing\n",
    "print('Missing instances of the dataset: %d' % sum(isnan(Xtrans).flatten()))\n",
    "\n",
    "no_missing_breast_DF = pd.DataFrame(Xtrans, columns = [ \n",
    "                    'Clump_Thickness', 'Uniformity_of_Cell_Size', 'Uniformity_of_Cell_Shape','Marginal_Adhesion', \n",
    "                    'Single_Epithelial_Cell_Size','Bare_Nuclei','Bland_Chromatin','Normal_Nucleoli','Mitoses','Class'])\n",
    "\n",
    "#after imputation\n",
    "no_missing_breast_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2 Data Integration\n",
    "Etant donné qu'on travaille sur un seul dataset, y'a pas besoin de faire cette étape\n",
    "\n",
    "## 3-3 Data Transformation\n",
    "Comme c'était le cas à partir de la partie de data understanding (section 2), on a été confronté à beaucoup de problemes, notamment sur la non distribution des données en loi gaussienne, aussi le probleme de imbalanced data car les labels classification ne sont pas equally representé.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imbalanced Data\n",
    "#Pour ce probleme on vas utiliser Oversampling\n",
    "from sklearn.utils import resample\n",
    "\n",
    "#create two different dataframe of majority and minority class \n",
    "df_majority = no_missing_breast_DF[(no_missing_breast_DF['Class'] == 2)] \n",
    "df_minority = no_missing_breast_DF[(no_missing_breast_DF['Class'] == 4)] \n",
    "\n",
    "# upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,    # sample with replacement\n",
    "                                 n_samples= 458, # to match majority class\n",
    "                                 random_state=42)  # reproducible results\n",
    "# Combine majority class with upsampled minority class\n",
    "breast_DF_balanced = pd.concat([df_minority_upsampled, df_majority])\n",
    "print(breast_DF_balanced['Class'].value_counts())\n",
    "\n",
    "print(\"Bar plot du dataset balanced(label=Class)\")\n",
    "sns.countplot(breast_DF_balanced['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian Distribution \n",
    "#LOG transformations\n",
    "\n",
    "print(\"Le Skew de chaque colonne avant la transformation:\")\n",
    "breast_DF_balanced.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "breast_DF_balanced.hist(color=\"r\", alpha=0.5, bins=50, figsize=(10,10))\n",
    "plt.title(\"Distribution des données de chaque colonne avant transformation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOG transformations\n",
    "breast_DF_balanced[\"Clump_Thickness\"] = breast_DF_balanced[\"Clump_Thickness\"].map(lambda x: log(x))\n",
    "breast_DF_balanced[\"Uniformity_of_Cell_Size\"] = breast_DF_balanced[\"Uniformity_of_Cell_Size\"].map(lambda x: log(x))\n",
    "breast_DF_balanced[\"Uniformity_of_Cell_Shape\"] = breast_DF_balanced[\"Uniformity_of_Cell_Shape\"].map(lambda x: log(x))\n",
    "breast_DF_balanced[\"Marginal_Adhesion\"] = breast_DF_balanced[\"Marginal_Adhesion\"].map(lambda x: log(x))\n",
    "breast_DF_balanced[\"Single_Epithelial_Cell_Size\"] = breast_DF_balanced[\"Single_Epithelial_Cell_Size\"].map(lambda x: log(x))\n",
    "breast_DF_balanced[\"Bare_Nuclei\"] = breast_DF_balanced[\"Bare_Nuclei\"].map(lambda x: log(x))\n",
    "breast_DF_balanced[\"Bland_Chromatin\"] = breast_DF_balanced[\"Bland_Chromatin\"].map(lambda x: log(x))\n",
    "breast_DF_balanced[\"Normal_Nucleoli\"] = breast_DF_balanced[\"Normal_Nucleoli\"].map(lambda x: log(x))\n",
    "breast_DF_balanced[\"Mitoses\"] = breast_DF_balanced[\"Mitoses\"].map(lambda x: log(x))\n",
    "\n",
    "print(\"Le Skew de chaque colonne apres la transformation:\")\n",
    "breast_DF_balanced.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "breast_DF_balanced.hist(color=\"b\", alpha=0.5, bins=50, figsize=(10,10))\n",
    "plt.title(\"Distribution des données de chaque colonne apres transformation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-4 Data Reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = sns.heatmap(breast_DF_balanced.corr(), annot = True)\n",
    "hm.set(xlabel='\\nCANCER Details', ylabel='CANCER Details\\t', title = \"Correlation matrix of BREAST CANCER data\\n\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de cette matrice de correlation, on apperçoit que les deux colonnes Uniformity_of_Cell_Size et Uniformity_of_Cell_Shape sont fortement corrélés 0.92, et qu'on peut retirer une des deux colonnes. Mais, puisque y'en a pas trop de données et de colonnes on peu aussi la garder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_DF_balanced = breast_DF_balanced.drop(['Uniformity_of_Cell_Size'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va retirer la colonne Uniformity_of_Cell_Size, car apres avoir supprimer cette colonne, le modele repond mieux avec une mielleur V-Measure de 0.8138934242085933(avec Kmeans) à 0.8261869877795835(avec Kmeans).<br>\n",
    "Pour ces raisons de performance du modele on vas la supprimer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-5 Data Discretization\n",
    "On a pas besoin de faire une discretization, car une Discretization c'est pour  transformer des variables en variable catégoriques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  4-   Measure the quality of the clusters methods\n",
    "Il y'a deux types de cas qu'on peut ce retrouvé à travailler avec afin de choisir les bonnes métriques pour vérifier la qualité des clusters.<br>\n",
    "Pour celà, on a:<br>\n",
    "Les datasets qui viennent avec leurs étiquettes(initially predicted labels).<br>\n",
    "Les datasets qui viennent sans leurs étiquettes(no initially predicted labels), qu'on prédit par la suite avec algorithms de clustering.<br>\n",
    "Nous dans notre cas sur le dataset BREAST CANCER, on se retrouve avec un dataset contenant déja des labeles initiaux, ce qui implique par la suite qu'on doit utiliser des métriques du qualité qui prennent en charge cette avantage d'avoir des labels initiaux, et de pouvoir les comparer avec les predictions ffaites par la suite. Parmi ces metriques on a:<br>\n",
    "<h2>4-1 Rand Index</h2><br>\n",
    "Compte tenu de la connaissance des affectations de classe de vérité terrain labels_true et de nos affectations d'algorithme de clustering des mêmes échantillons labels_pred , l' indice de Rand (ajusté ou non) est une fonction qui mesure la similitude des deux affectations, en ignorant les permutations.\n",
    "<br>\n",
    "<h2>4-2 Mutual information based scores</h2><br>\n",
    "Compte tenu de la connaissance des affectations de classe de vérité terrain labels_true et de nos affectations d'algorithme de clustering des mêmes échantillons labels_pred , l' information mutuelle est une fonction qui mesure l' accord des deux affectations, en ignorant les permutations. Deux versions normalisées différentes de cette mesure sont disponibles, les informations mutuelles normalisées (NMI) et les informations mutuelles ajustées (AMI) . Le NMI est souvent utilisé dans la littérature, tandis que l'AMI a été proposé plus récemment et est normalisé contre le hasard.\n",
    "<br>\n",
    "<h2>4-3 Homogeinity, completeness and V-Measure</h2><br>\n",
    "En connaissant les affectations de classe de la vérité du sol des échantillons,il est possible de définir une métrique intuitive en utilisant l'analyse de l'entropie conditionnelle.<br>\n",
    "\n",
    "En particulier,Rosenberg et Hirschberg (2007)définissent les deux objectifs souhaitables suivants pour toute affectation de cluster :<br>\n",
    "\n",
    "homogénéité : chaque cluster ne contient que les membres d'une même classe.\n",
    "complétude : tous les membres d'une classe donnée sont affectés au même cluster.\n",
    "Nous pouvons transformer ces concepts en scores homogeneity_score et completeness_score . Les deux sont limités en-dessous par 0,0 et au-dessus par 1,0 (plus c'est mieux).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  5-  The TWO Clustering algorithms\n",
    "Apres avoir préparer les données dans la partie de preprocessing de données, on va maintenant implimenté deux algorithms de clustering pour faire de forecasting\n",
    "## 5-1 KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "x = breast_DF_balanced.drop(['Class'], axis=1)\n",
    "y = breast_DF_balanced['Class']\n",
    "\n",
    "\n",
    "#Finding the optimum number of clusters for k-means classification:La méthode du coude \n",
    "#nous permet de choisir la quantité optimale de clusters pour la classification. \n",
    "#Bien que nous sachions déjà que la réponse est 2, il est toujours intéressant de le voir.\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "\n",
    "for i in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n",
    "    kmeans.fit(x)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "#Plotting the results onto a line graph, allowing us to observe 'The elbow'\n",
    "plt.plot(range(1, 10), wcss)\n",
    "plt.title('The elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS') #within cluster sum of squares\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying kmeans to the dataset / Creating the kmeans classifier\n",
    "#A partir de elbow method on prend K=2\n",
    "\n",
    "kmeans = KMeans(n_clusters = 2, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n",
    "y_kmeans = kmeans.fit(x)\n",
    "\n",
    "y = y.astype(int)\n",
    "y = y.replace(4,1)\n",
    "y = y.replace(2,0)\n",
    "\n",
    "import numpy as np\n",
    "color =np.array(['red','green'])\n",
    "#adding the colors\n",
    "plt.scatter(x=breast_DF_balanced.Bare_Nuclei#Uniformity_of_Cell_Size,\n",
    "            ,y=breast_DF_balanced.Uniformity_of_Cell_Shape,c=color[y])\n",
    "plt.title(\"The actual dataset\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After the clustering\n",
    "#adding the colors\n",
    "color2=np.array(['green','red'])\n",
    "plt.scatter(x=breast_DF_balanced.Bare_Nuclei#Uniformity_of_Cell_Size\n",
    "            ,y=breast_DF_balanced.Uniformity_of_Cell_Shape,c=color2[y_kmeans.labels_])\n",
    "plt.title(\"The dataset post clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "labels_true = y\n",
    "labels_pred = y_kmeans.labels_\n",
    "print(\"Homogenity du clustering\", metrics.homogeneity_score(labels_true, labels_pred))\n",
    "print(\"Completeness du clustering\",metrics.completeness_score(labels_true, labels_pred))\n",
    "print('Leur moyenne harmonique appelée V-mesure est calculée par v_measure_score :',metrics.v_measure_score(labels_true, labels_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-2 Gaussian Mixture with Expectation Maximization (EM) Clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Mixture with Expectation Maximization (EM) Clustering\n",
    "# Uses all specified components to fit.\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "gm = GaussianMixture(n_components=2, covariance_type=\"full\")\n",
    "gm_pred = gm.fit_predict(x)\n",
    "\n",
    "# Scatter plots\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(15,5))\n",
    "\n",
    "ax1.scatter(x=breast_DF_balanced.Bare_Nuclei,y=breast_DF_balanced.Uniformity_of_Cell_Shape, c=y, cmap=\"brg\", edgecolor=\"None\", alpha=0.35)\n",
    "ax1.set_title(\"Actual clusters\")\n",
    "\n",
    "ax2.scatter(x=breast_DF_balanced.Bare_Nuclei,y=breast_DF_balanced.Uniformity_of_Cell_Shape, c=gm_pred, cmap=\"brg\", edgecolor=\"None\", alpha=0.35)\n",
    "ax2.set_title(\"Gaussian Mix-EM clustering plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "labels_true = y\n",
    "labels_pred = gm_pred\n",
    "print(\"Homogenity du clustering\", metrics.homogeneity_score(labels_true, labels_pred))\n",
    "print(\"Completeness du clustering\",metrics.completeness_score(labels_true, labels_pred))\n",
    "print('Leur moyenne harmonique appelée V-mesure est calculée par v_measure_score :',metrics.v_measure_score(labels_true, labels_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6- Kmeans VS Gaussian Mixture with Expectation Maximization (EM) Clustering\n",
    "On va comparer  dans cette section la qualité des deux algorithms en utilisant les trois differentes metrics (meme ci une est largement suffisante), mais c'est pour voir c'est y'a vraiment des grands changements avec ces métrics.<br>\n",
    "Aussi, on va utilisé silhouette qui est une métrique qu'on a pas décrit la dessus, mais c'est une métrique pour le cas ou on connais pas les labels initiaux de dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "algorithms = []\n",
    "algorithms.append(kmeans)\n",
    "algorithms.append(gm)\n",
    "\n",
    "data = []\n",
    "for algo in algorithms:\n",
    "    algo.fit(x)\n",
    "    if(algo == kmeans):\n",
    "        data.append(({\n",
    "            'ARI': metrics.adjusted_rand_score(y, algo.labels_),\n",
    "            'AMI': metrics.adjusted_mutual_info_score(y, algo.labels_,\n",
    "                                                     average_method='arithmetic'),\n",
    "            'Homogenity': metrics.homogeneity_score(y, algo.labels_),\n",
    "            'Completeness': metrics.completeness_score(y, algo.labels_),\n",
    "            'V-measure': metrics.v_measure_score(y, algo.labels_),\n",
    "            'Silhouette': metrics.silhouette_score(x, algo.labels_)}))\n",
    "    else:\n",
    "            data.append(({\n",
    "            'ARI': metrics.adjusted_rand_score(y, algo.fit_predict(x)),\n",
    "            'AMI': metrics.adjusted_mutual_info_score(y, algo.fit_predict(x),\n",
    "                                                     average_method='arithmetic'),\n",
    "            'Homogenity': metrics.homogeneity_score(y, algo.fit_predict(x)),\n",
    "            'Completeness': metrics.completeness_score(y, algo.fit_predict(x)),\n",
    "            'V-measure': metrics.v_measure_score(y, algo.fit_predict(x)),\n",
    "            'Silhouette': metrics.silhouette_score(x, algo.fit_predict(x))}))\n",
    "        \n",
    "results = pd.DataFrame(data=data, columns=['ARI', 'AMI', 'Homogenity',\n",
    "                                           'Completeness', 'V-measure', \n",
    "                                           'Silhouette'],\n",
    "                       index=['K-means', 'Gaussian Mixture with EM'])\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir des résultats obtenus, on peu conclure que le meilleure model pour cette éxperience sur le Breast Cancer Dataset est KMeans. Ou on  peu voir, il a legerement le dessus sur GM sur toutes les mésures de qualité vu et éffectué sur ces deux modeles.<br>\n",
    "Celà conclut, qu'on doit prendre KMeans que GM(EM), car il donne des meilleures résultats que çà soit avec les labels initiaux de dataset ou sans, il est toujours devant plus précis que l'autre algorithme de clustering "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
